{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"lda_task.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"yyhzhgsXRua3"},"source":["# Лабораторная работа № 4. Латентное размещение Дирихле"]},{"cell_type":"code","metadata":{"id":"cyqzodonRua5"},"source":["import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24wlkjssRua8"},"source":["В этой лабораторной работе мы будем работать со [сценариями кинофильмов на английском языке](https://www.kaggle.com/jrobischon/wikipedia-movie-plots)\n","\n","Загрузите данные и положите их в ту же папку, что и ноутбук с заданием."]},{"cell_type":"code","metadata":{"id":"NZMrR3HTRua8"},"source":["rawdata = pd.read_csv(\"wiki_movie_plots_deduped.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4DbAixERua_"},"source":["Прежде чем строить тематическую модель, нужно подготовить данные. Сейчас документы выглядят вот так:"]},{"cell_type":"code","metadata":{"id":"Pk0sjazDRua_"},"source":["for plot in rawdata.Plot.sample(2):\n","    print(plot, \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVjBDby6RubB"},"source":["В тематической модели LDA порядок слов в документе никак не учитывается, важно лишь какие слова и сколько раз встречались в документе. Поэтому далее для обучения модели мы будем использовать представление текста в виде [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n","\n","Для начала нужно выкинуть из текстов все лишнее, кроме слов: пунктуацию, числа и т.п. Для этого применим стандартную процедуру [токенизации](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization): \n","* разобьем исходные документы-строки на токены\n","* отбросим все токены, кроме слов \n","* соединим токены-слова обратно в строку, для удобства хранения (списки в Python занимают много памяти)"]},{"cell_type":"code","metadata":{"id":"YY5Qx25VRubC"},"source":["from nltk.tokenize import word_tokenize\n","\n","data = rawdata.Plot.apply(lambda x: \" \".join(list(filter(str.isalpha, word_tokenize(x.lower())))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F8Y1Ar3qRubE"},"source":["Теперь приведем все слова к начальной форме, чтобы уменьшить размер словаря. Это делается с помощью процедуры [лемматизации](https://en.wikipedia.org/wiki/Lemmatisation)"]},{"cell_type":"code","metadata":{"id":"rZHozsh0RubE"},"source":["from nltk.stem import WordNetLemmatizer \n","  \n","lemmatizer = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T37nLOihRubG"},"source":["data = data.apply(lambda x: \" \".join([lemmatizer.lemmatize(w) for w in x.split()]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPC6yPFHRubI"},"source":["Вот как теперь выглядят наши тексты"]},{"cell_type":"code","metadata":{"id":"R8T8ljpBRubI"},"source":["print(data.sample(1).values[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_jAHoRgRubK"},"source":["Воспользуемся [библиотекой для визуализации текстовых данных](https://github.com/amueller/word_cloud), чтобы нагляднее посмотреть, из каких слов состоят документы"]},{"cell_type":"code","metadata":{"id":"buagF5tbRubL"},"source":["from wordcloud import WordCloud\n","\n","all_texts = ','.join(list(data.values))\n","wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, \n","                      contour_color='steelblue', width=800, height=400)\n","\n","wordcloud.generate(all_texts)\n","wordcloud.to_image()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mdhVxo95RubM"},"source":["Теперь создадим bag of words. При его создании мы не будем добавлять в словарь стоп-слова -- служебные части речи, которые присутствуют в подавляющем большинстве текстов, и потому не несут значимой информативной нагрузки"]},{"cell_type":"code","metadata":{"id":"qXWovFU2RubN"},"source":["import nltk\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english') + ['ha', 'wa'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ZTXnTzzRubP"},"source":["В стоп-слова добавлены 'ha' и 'wa' потому что в них лемматизатор превращает слова has и was"]},{"cell_type":"markdown","metadata":{"id":"8sfHF9bmRubP"},"source":["Воспользуемся [BOW из sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Слова, которые встречаются слишком редко (в небольшом числе документов) мы тоже отбросим, чтобы не перегружать словарь"]},{"cell_type":"code","metadata":{"id":"zkFYObHpRubQ"},"source":["from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8p5z158URubS"},"source":["count_vectorizer = CountVectorizer(stop_words=stop_words, min_df=10)\n","count_data = count_vectorizer.fit_transform(data.values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6wD7vDCcRubT"},"source":["Посмотрим на распределение слов в нашем корпусе текстов"]},{"cell_type":"code","metadata":{"id":"VzHtRJ-2RubU"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_style('whitegrid')\n","%matplotlib inline\n","\n","def plot_most_common_words(count_data, count_vectorizer, n_words):\n","    words = count_vectorizer.get_feature_names()\n","    total_counts = np.zeros(len(words))\n","    for t in count_data:\n","        total_counts+=t.toarray()[0]\n","    \n","    count_dict = (zip(words, total_counts))\n","    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:n_words]\n","    words = [w[0] for w in count_dict]\n","    counts = [w[1] for w in count_dict]\n","    x_pos = np.arange(len(words)) \n","    \n","    plt.figure(2, figsize=(15, 15/1.6180))\n","    plt.subplot(title='Most common words')\n","    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n","    sns.barplot(x_pos, counts, palette='husl')\n","    plt.xticks(x_pos, words, rotation=90, fontsize=15) \n","    plt.xlabel('words')\n","    plt.ylabel('counts')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1KHPDNc3RubW"},"source":["plot_most_common_words(count_data, count_vectorizer, 20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MtzwsrO5RubY"},"source":["## Стандартная модель LDA\n","\n","Теперь приступим к обучению модели. Для начала обучим стандартную модель LDA.\n","\n","**Задание 1**\n","\n","Изучите [модель LDA из sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) и обучите её на подготовленных данных. Посмотрите на распределения слов в темах, выведите наиболее вероятные слова для каждой темы"]},{"cell_type":"markdown","metadata":{"id":"W1vZKhGkRubY"},"source":["Вспомагательная функция для визуализации самых вероятных слов в темах:"]},{"cell_type":"code","metadata":{"id":"rl81OrlVRubZ"},"source":["def print_topics(count_vectorizer, phi_matrix, n_top_words):\n","    words = count_vectorizer.get_feature_names()\n","    for topic_idx, topic in enumerate(phi_matrix):\n","        print(\"\\nTopic #{}\".format(topic_idx))\n","        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFvdEKIrRuba"},"source":["### YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnaXLioyRubd"},"source":["**Задание 2** \n","\n","Часто бывает так, что в LDA выделяется одна или несколько тем с общей лексикой -- т.е. со словами, которые часто встречаются в большом количестве документов корпуса. Проанализируйте полученные в LDA темы и найдите в выделенных темах те, в которые собрались слова общей лексики (или покажите, что таких тем нет). Для этого можно, например:\n","* проанализировать встречаемость каждой темы в документах, посмотреть на среднее/дисперсию/распределение вероятностей каждой темы в корпусе документов\n","* посмотреть на встречаемость самых частых слов корпуса в темах"]},{"cell_type":"code","metadata":{"id":"T4thfVAQRube"},"source":["### YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tjcNUJeRubi"},"source":["## Упрощенная модель LDA (у каждого документа только одна тема)\n","\n","Теперь обучим упрощенную модель LDA, разобранную на семинаре. В данной модели одному документу соответсвует одна тема. Вероятностная модель выглядит следующим образом:\n","\n","Случайные величины:\n","* $w_{dn}$ -- слово, стоящее на $n$-ой позиции в документе $d$ (наблюдаемая случайная величина)\n","* $t_d$ -- тема документа $d$ (латентная случайная величина)\n","\n","Параметры модели:\n","* $\\Phi = \\{\\phi_{tw}\\} \\in \\mathbb{R}^{T\\times V}$ -- матрица распределений слов по темам  ($T$ -- число тем, $V$ -- размер словаря)\n","* $\\pi \\in \\mathbb{R}^T$ -- вектор распределения тем в корпусе документов \n","\n","Совместное распределение на слова в документах и темы документов задается следующим образом:\n","\n","$p(W, t \\;|\\; \\Phi, \\pi) = p(W \\;|\\; t, \\Phi) p(t \\;|\\; \\pi) = \\prod_{d=1}^D \\prod_{n=1}^{N_d}p(w_{dn}\\;|\\;t_d, \\Phi) p(t_d \\;|\\; \\pi) = \\prod_{d=1}^D \\prod_{n=1}^{N_d} \\phi_{t_d w_{dn}}\\pi_{t_d}$\n","\n","Поскольку в модели присутствуют латентные переменные, оптимизацию параметров мы будем вести с помощью ЕМ-алгоритма:\n","* **E-шаг:**  $KL(q(t) \\;||\\; p(t\\;|\\;\\Phi, \\pi) ) \\to \\min_{q(t)}$\n","* **M-шаг:** $\\mathbb{E}_{q(t)} \\log p(W, t \\;|\\; \\Phi, \\pi) \\to \\max_{\\Phi, \\pi}$\n","\n","**Задание 3**\n","\n","Выведите формулы для нахождения $q(t_d = t) = \\mu_{dt},\\;\\Phi,\\;\\pi$ и оптимизируемого функционала ELBO"]},{"cell_type":"code","metadata":{"id":"E96hWnkfRubi"},"source":["### YOUR FORMULAS HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fW-gxMqpRubk"},"source":["**Задание 4**\n","\n","Реализуйте упрощенный LDA\n","\n","**Подсказка:** Если вам нужно будет посчитать $\\log Softmax$, лучше воспользуйтесь устойчивым способом подсчета из второй практической работы:\n","\n","$$\\alpha_i = \\log{p_i(\\dots)} \\quad\\rightarrow \\quad\n","\t\\frac{e^{\\alpha_i}}{\\sum_k e^{\\alpha_k}} = \n","\t\\frac{e^{(\\alpha_i - \\max_j \\alpha_j)}}{\\sum_k e^{(\\alpha_k- \\max_j \\alpha_j)}}$$"]},{"cell_type":"code","metadata":{"id":"1Jm89CRrRubl"},"source":["class SimpleLDA(object):\n","    def __init__(self, n_topics, epsilon=1e-15, tol=1e-5):\n","        self.n_topics = n_topics\n","        self.epsilon = epsilon\n","        self.tol = tol\n","        \n","    def fit(self, bow, verbose=True):\n","        self._initialize(bow)\n","        elbo = -np.inf\n","        for it in range(1000):\n","            self._e_step(bow)\n","            self._m_step(bow)\n","            new_elbo = self._count_elbo(bow)\n","            diff = new_elbo - elbo\n","            if verbose:\n","                print(\"\\n{}:\\n elbo: {}\\n increase: {}\".format(it, new_elbo, diff))\n","            if diff < self.tol:\n","                break\n","            elbo = new_elbo\n","            \n","    \n","    def _initialize(self, bow):\n","        V = bow.shape[1]\n","        self.mu = None\n","        self.pi = np.abs(np.random.randn(self.n_topics))\n","        self.pi = self.pi / self.pi.sum()\n","        self.phi = np.abs(np.random.randn(self.n_topics, V))\n","        self.phi = self.phi / self.phi.sum(axis=1)[:, np.newaxis]\n","        \n","    def _e_step(self, bow):\n","        ### YOUR CODE HERE ###\n","        \n","    def _m_step(self, bow):\n","        ### YOUR CODE HERE ###\n","        \n","    def _count_elbo(self, bow):\n","        elbo = ### YOUR CODE HERE ###\n","        return elbo"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpJQrMLTRubm"},"source":["**Задание 5**\n","\n","Обучите упрощенный LDA и посмотрите на самые вероятные слова в каждой теме. Насколько различные получились темы? Насколько интерпретируемые? Сравните их с темами, полученными в классической модели LDA. "]},{"cell_type":"code","metadata":{"id":"6tHfJpNORubo"},"source":["### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y39ayJ9gRubq"},"source":["\n","**Задание 6**\n","\n","У упрощенной модели LDA могли возникнуть проблемы из-за слов общей лексики. Попробуйте отбросить из словаря слова, которые встречаются больше, чем в 10% документов и обучить упрощенный LDA на этих данных. Посмотрите на получившиеся темы. Как они отличаются от тем модели, обученной на словаре с общей лексикой? Чем объясняются эти различия? Почему у стандартной модели LDA возникает меньше проблем со словами общей лексики?"]},{"cell_type":"code","metadata":{"id":"Sn4XT__hRubr"},"source":["### YOUR CODE HERE ###"],"execution_count":null,"outputs":[]}]}